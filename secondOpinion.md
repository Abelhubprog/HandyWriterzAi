HandyWriterzAI System Integration Restoration Plan
Category 1: API Proxy & Routing Breakdown
Explanation: The Next.js frontend’s API proxy layer is misconfigured, causing request routing failures. For example, the server route handler uses a hardcoded logic for the BACKEND_URL that doesn’t adapt in production deployments (see frontend/src/app/api/chat/route.ts, lines 52-59). Additionally, the Next.js API route expects a trace_id in the backend response but the backend may only return a conversation_id, leading to a missing-ID error (lines 91-101 in route.ts). The custom API client further exacerbates this: its chat() method calls the backend /api/chat and then tries to stream from /api/stream/${traceId}, which might not match any backend endpoint. These mismatches result in CORS issues or 404 errors, leaving the UI stuck on “Processing…”. Fix Strategy: We will unify the API contract and environment configuration between the frontend and backend to repair the proxy layer. In practice, this means using a single source of truth for the backend URL and consistent endpoint naming. The Next.js route handler will be updated to handle conversation IDs robustly and not assume a trace_id field. We’ll also align the streaming endpoint reference: the frontend should request the exact path the backend serves (e.g. use conversation_id in the URL, or update the backend to accept traceId universally). Furthermore, we will choose a single SSE connection strategy – either always via the Next.js API proxy or direct to backend – to avoid duplicate connections. This unified approach ensures the proxy won’t break due to env misconfiguration and that the chat flow always obtains the correct ID for streaming. TODO (API Proxy Fixes):
Unify environment config: In frontend/src/app/api/chat/route.ts, refactor the backend URL resolution logic (around lines 52-59) to use a single environment variable. Remove any hardcoded or fallback logic that fails in non-development environments. Ensure that in production, the Next.js API route correctly points to the deployed FastAPI backend (e.g. via process.env.BACKEND_URL) and that the client does not bypass this route unintentionally.
Handle conversation ID in responses: In frontend/src/app/api/chat/route.ts, update the response handling (around lines 91-109) so it doesn’t crash if trace_id is absent. For example, use conversation_id (the new unified identifier) as the primary key. If the backend returns only a conversation_id, forward that to the client. Remove the forced error on missing trace_id, and instead ensure at least one ID is present (after backend fixes, it will always be). This prevents 500 errors caused by the proxy expecting the wrong field.
Align streaming endpoint names: Adjust the frontend’s usage of the stream URL to match the backend. In frontend/src/services/advancedApiClient.ts (e.g. its chat() or streaming method, lines ~331-337), replace any references to /api/stream/${traceId} with the correct path using conversation_id (e.g. /api/stream/${conversationId}). Likewise, ensure that the Next.js Route Handler for streaming (if one exists) or the client hook uses the same convention. If we keep the Next.js SSE proxy route, have the client call "/api/stream/[id]" (relative path) instead of constructing a full URL with possibly mismatched domains. This guarantees the SSE request hits the correct backend endpoint.
Choose SSR proxy vs direct SSE: Decide on a consistent SSE connection approach. For production, the safest route is using Next.js server as a proxy (to avoid CORS). Implement or fix frontend/src/app/api/stream/[id]/route.ts (if present) to simply forward the stream bytes from BACKEND_URL. Then modify the client’s stream hook to connect to this relative API route. Alternatively, if using direct browser SSE, ensure NEXT_PUBLIC_API_BASE_URL is identical to BACKEND_URL and remove the Next.js proxy route to avoid confusion. Do not use both – pick one pipeline and remove the other to prevent “competing connections”.
Verify end-to-end ID propagation: After the above changes, test that when the frontend calls the chat API route, it receives a conversationId (or traceId unified) and then uses that same ID for the SSE endpoint. The conversation identifier should be consistent across the POST response and the SSE URL on both frontend and backend. This will resolve the ID mismatches (traceId vs conversation_id) that caused 404s.
Category 2: SSE Streaming Pipeline Failures
Explanation: The Server-Sent Events pipeline between the backend and frontend is broken at multiple points. On the frontend, the streaming hook constructs the wrong URL and expects a different data format: it uses ${backendUrl}/api/stream/${traceId} (with traceId) instead of the conversation ID the backend actually needs. As a result, the SSE request never reaches the correct endpoint (leading to 404s). Even if it did, the frontend parsing logic looks for fields like delta or text in event data that the backend doesn’t send. On the backend side, no SSE events are actually being broadcast because the SSE publisher is never initialized – the global sse_publisher remains None and initialize_sse_publisher() is never called. The UnifiedProcessor tries to publish events through this null publisher (and sometimes directly to Redis channels), but since the frontend isn’t subscribed to Redis, those events vanish. There’s even a “split-brain” in channel usage: parts of the code use a different channel naming (sse:unified:{id}) versus the legacy sse:{conversation_id}, causing confusion about where events go. Overall, these issues mean no real-time updates reach the browser at all. Fix Strategy: We need to establish a unified, functioning SSE pipeline that streams real-time events from the multi-agent backend to the UI. This involves several coordinated fixes. First, initialize the SSE publisher on startup so the backend can actually emit events (e.g. instantiate an SSEService or call the publisher init in main.py). Next, standardize the SSE channel and endpoint: use a single naming convention (likely conversation_id) for SSE URLs and Redis channels to eliminate traceId vs convoId mismatches. We will update the backend to publish all events to the channel sse:{conversationId} and ensure the /api/stream/{conversation_id} endpoint subscribes to that same channel. The frontend’s useStream hook will be corrected to request /api/stream/{conversationId} (matching the backend path). Additionally, we must align the SSE event schema across front and back. We’ll standardize on a JSON structure with a clear type field and message content, matching the ChatGPT-style stream format. For example, instead of the backend sending { event_type: "content", data: { content: "text..." } }, it should send { type: "content", message: "text..." } – or we adjust the frontend to handle the backend’s structure. Our plan is to update the backend to use a consistent SSEEvent model for all events, and modify the frontend parser to switch on data.type (like “token”, “content”, “research_progress”, etc.). This will resolve the current parsing failures. Finally, we will add robust error handling for the SSE connection: if streaming fails or disconnects, the UI will be notified gracefully instead of hanging indefinitely. By implementing these fixes, the SSE pipeline will reliably deliver real-time multi-agent updates to the client. TODO (SSE Pipeline Fixes):
Initialize SSE publisher on startup: In backend/src/main.py, ensure the SSE publishing system is started when the app launches. For example, import and instantiate the SSE publisher/singleton at application startup. If a lifespan context is used in FastAPI, call initialize_sse_publisher() or create a new SSEService.get_instance() within the startup section. Confirm that the global publisher (or new SSEService) is set and ready before any chat requests are handled. (File & Line: Add this in app startup, e.g. around main.py line 115 after creating the FastAPI app, or within the lifespan async context if present.)
Unify SSE channel naming: Standardize all SSE pub/sub channels to use the conversation ID. In backend/src/agent/routing/unified_processor.py, remove any usage of a separate sse:unified:{id} channel. Modify the internal _publish_event logic (around lines 351-386) to publish to the default channel sse:{conversation_id} via the now-initialized SSE service. Likewise, check if the SSE subscription endpoint uses the same format: in backend/src/main.py, the @app.get("/api/stream/{conversation_id}") handler currently subscribes to f"sse:{conversation_id}" – keep that, and ensure all publishing goes into that channel. This alignment means the backend will consistently broadcast on sse:<conversationId> and the stream endpoint will receive those events (resolving the prior “split-brain” issue).
Fix frontend SSE endpoint usage: Update the client to request the correct streaming URL. In frontend/src/hooks/useStream.ts, construct the SSE URL using the conversation ID that the chat API returned. For example, use const sseUrl = `/api/stream/${conversationId}`; (if using the Next.js proxy route) or ensure the backendUrl it uses equals the backend host and the path is /api/stream/{conversation_id}. Remove any lingering usage of traceId in the path. This change paired with the backend endpoint fix will eliminate the 404 on streaming attempts.
Align event schema and parsing: Refactor how SSE events are produced and consumed:
Backend: In backend/src/agent/routing/unified_processor.py (and related agent nodes), use a unified event schema. Replace ad-hoc JSON structures with a consistent model. For instance, utilize an SSEEventFactory to create events with a type field and relevant data. Ensure that token streams, agent reasoning updates, and final outputs are packaged in this format. (If needed, implement helper methods like publish_content_token() in an SSEService class to send token-by-token updates easily.)
Frontend: In frontend/src/hooks/useStream.ts, modify the SSE onmessage handler to parse the JSON event and interpret it by data.type. For example:
If data.type === 'content', append data.message to the streaming buffer (this replaces relying on data.delta or data.text, which are no longer used).
If data.type === 'research_progress' (for example), update the UI’s state for research steps (mapping from what backend sends, e.g. backend might send type: 'research' with details which we convert accordingly).
Handle all expected event types (e.g. planning_started, agent_thought, done) consistently.
By doing this, the frontend will correctly render the multi-agent process (e.g. partial content streaming, progress events) instead of being stuck waiting for nonexistent fields.
Implement SSE error handling: Enhance both backend and frontend to handle SSE failures gracefully. On the frontend, update the useStream hook to listen for error events (EventSource.onerror). If the stream fails to connect or drops, trigger a state update (e.g. set a flag streamError or dispatch an error message to the UI). Also consider a timeout: if no first message is received in a reasonable time, auto-close and report an error to the user. On the backend, wrap SSE publishing in try/except and use timeouts on Redis subscribe. If an exception occurs or no events are published (e.g. graph fails), ensure a final SSE event or HTTP response indicates completion or error, so the frontend isn’t left hanging. Adding these error boundaries will prevent indefinite “Processing…” states and make debugging easier.
Test real-time flow end-to-end: After fixes, perform a full integration test: Send a chat message and confirm that:
The backend logs show SSE events being published (no errors about None publisher).
The SSE endpoint returns a stream of events to the browser (verify the network stream in dev tools).
The UI updates incrementally (tokens of the AI’s response appear one by one, and any agent progress indicators show up as designed).
Induce an error (e.g. stop the backend or cause a graph exception) to ensure the frontend error handling displays a message instead of freezing.
This will validate that the SSE pipeline is robust and delivering real-time multi-agent chat updates.
Category 3: Agent Orchestration & State Gaps
Explanation: There are disconnects between the multi-agent backend “brain” and the UI flow. On the frontend, the chat logic doesn’t properly wait for the backend’s confirmation before starting to stream. The useAdvancedChat hook (or related logic) calls the API client and then immediately opens an SSE connection without ensuring the backend is ready (see line 223 in useAdvancedChat.ts where it starts streaming right after sending the request). If the conversationId (trace ID) hasn’t been obtained yet, the SSE may connect to the wrong endpoint or too early. Moreover, if the SSE never connects, the current code’s error handling (line 244) is inadequate, assuming success and not recovering when none comes. On the backend, the LangChain-like graph execution is not fully integrated. In backend/src/agent/routing/unified_processor.py, it invokes the handywriterz_graph.ainvoke() to execute the agent workflow (line 566), but there is no check that the graph is initialized or returns valid output. The code later tries to extract content from the graph result (lines 589-621) assuming a structured response, which may be None if the graph didn’t produce output, causing downstream failures. In addition, uploaded files are not actually used in the agent’s context: file uploads succeed, but the content never reaches the agents. The code references a File Content service that is not implemented, meaning any file-based context is effectively lost. This breaks features like attaching papers or data for the AI to use. Finally, the demo interface (DemoReadyChatInterface) isn’t wired into the real backend at all – it short-circuits the flow (simulating responses) and doesn’t handle errors (e.g. it calls sendMessage() but never handles a failure if the backend is down). There’s also evidence of stale code references, such as a missing ChatPane component in the import structure, which would cause runtime errors if any part of the app tries to render it. Fix Strategy: We will bridge the gaps in the agent orchestration and UI state management to ensure a seamless chat experience. On the frontend side, the chat hook should be refactored to coordinate with the backend’s response: it must wait for the initial chat request to return a conversation ID (or acknowledgement) before initiating the SSE stream. For example, after calling apiClient.chat(), the code should only open the EventSource once the trace_id/conversation_id is known and valid. We’ll implement that sequencing and improve error handling in the hook so that if SSE never connects, the UI will know and can display an error (rather than silently failing). On the backend side, we need to tighten the integration of the LangGraph (agent graph) execution. We should ensure the graph is loaded/initialized at app start (or handle lazy init) and verify that handywriterz_graph.ainvoke() is successful. Adding checks around line 566 in unified_processor.py (and around line 261 where advanced processing is set up) will allow us to log or handle cases where the graph is unavailable. Moreover, to achieve real-time token streaming from the agent, the graph’s nodes (especially the final text-generating node) should emit partial results to SSE. We will integrate the SSE publisher within the graph’s workflow – for example, as the “Writer” agent generates content, it can call an SSEService.publish_content_token() on each token or sentence. This means modifying the graph’s nodes or the unified processor to push interim results (e.g. implementing a method like stream_tokens in the writer node to publish tokens gradually). This real-time orchestration ensures the frontend receives tokens as the AI “thinks” and writes, rather than only a final blob at the end. We also must handle file uploads in the agent pipeline. The fix is to implement the missing File Content Service or equivalent logic so that when a file is uploaded, its contents are parsed and attached to the conversation state or passed into the graph input. We’ll likely create a new module (e.g. services/file_content.py) that stores the file (perhaps in a database or memory) and returns an ID or text content. Then, in the chat workflow (possibly in main.py or in unified_processor.py where file inputs are handled), retrieve the file’s text and supply it to the graph or agents that require it. This will enable agents to actually utilize uploaded files (e.g. for citations or context) rather than ignoring them. On the UI, we will improve the state management and error feedback. This includes adding proper loading and error states in the chat interface (e.g. a message if the graph fails or if no response is received in some time). The demo interface can be updated to use the real backend by default (to avoid divergence), or clearly indicate it’s in demo mode. We will also remove or update any broken references such as ChatPane – ensuring all components in use exist and are correctly imported, preventing runtime crashes. By executing these changes, the multi-agent orchestration will be tightly coupled with the UI: conversation IDs flow through, the backend’s reasoning steps are streamed live, file context is included, and the UI remains in sync with robust error handling. TODO (Orchestration & UI Fixes):
Synchronize chat start and streaming: In frontend/src/hooks/useAdvancedChat.ts (or the relevant hook/component that sends the message), refactor the flow around where sendMessage is called. For instance, at line 172 and 223, instead of immediately starting useStream after calling apiClient.chat(...), wait for the promise to resolve and yield a conversationId. Implement logic such as:
Call the chat POST route; get the result JSON.
Extract conversation_id from the result.
Only then call the streaming hook or EventSource with that ID.
This ensures the SSE connects with a valid conversation ID and after the backend has set up any needed state. Also, handle the case where the backend response lacks an ID or returns an error: in that case, do not open SSE at all, but surface an error to the user (e.g. “Unable to start chat, please try again”). This change will prevent racing ahead of the backend and fix the current blind SSE start issue.
Improve frontend error handling & state: Still in the chat hook/component, add handling for SSE errors and timeouts. For example, use the onerror of EventSource or a timeout (say 30s) to detect if no messages arrived. When triggered, update the application state (perhaps via a React state or context) to indicate an error in streaming. Also consider providing a cancel/retry mechanism: if a chat is stuck, allow the user to cancel that conversation attempt. Additionally, update the UI elements (e.g. disable the “Send” button while waiting, show a spinner or “Processing…” with a cancel option). These changes will make the UI more resilient if the backend still fails for any reason.
Ensure graph execution starts and streams: In backend/src/agent/routing/unified_processor.py, address the gaps around the LangChain graph invocation:
Around line 566, add a check or log before calling handywriterz_graph.ainvoke(state, config). If the graph isn’t initialized (maybe via a global or a config flag), initialize or load it here, or return an error SSE event that the service is unavailable. This prevents silent failures if handywriterz_graph was None.
After invoking, ensure that as the graph runs, it streams events. If the graph API allows callbacks or yielding of intermediate results, hook those into our SSE publisher. For example, if the graph’s nodes can call back on each token or each agent action, connect those to get_sse_service().publish_event(...) with appropriate event types. If not inherently supported, modify key agent nodes (like the final content generation) to manually emit tokens. You might create a method in backend/src/agent/nodes/writer.py (or equivalent) to split the final content into tokens and send them out as they are generated. Use a short asyncio.sleep between tokens for realism if needed. The goal is to have the backend push multiple SSE events (planning started, research progress, partial content, etc.) during the ainvoke execution, rather than waiting until everything is done.
Around lines 589-621 in unified_processor.py, add safe checks when processing the graph result. If result or expected fields are null, handle it gracefully: maybe send an SSE "error" event or at least avoid attribute errors. Also finalize the SSE stream properly: once the graph workflow is complete, send a "done" event or similar so the frontend knows to stop listening.
With these adjustments, the backend will reliably execute the agent graph and communicate its progress live.
Implement file content integration: Create or complete the file content service so that uploaded files influence the chat. For example, implement backend/src/services/file_service.py with functions to save file content (perhaps storing text in a database or in-memory dict) and retrieve it. In backend/src/main.py, find the file upload endpoint or the part of the chat processing that references file content (the failure report suggests this is around line 1254 in main). Modify that code to use the new file service: when a file is uploaded, immediately parse its text (e.g. PDF to text, or assume text file) and store it via the service. Then, attach an identifier or the text itself to the conversation state. This might mean extending the conversation model or state passed into unified_processor. Ensure that when the graph is invoked, it has access to this file content – for instance, include it in the config or initial prompt construction. After implementing, test by uploading a file and sending a query that requires it; the agents should utilize the content (you might see events like file processing start/complete, which you can also stream to the UI for feedback).
Fix the demo interface and component issues: In frontend/src/components/chat/DemoReadyChatInterface.tsx, adjust the component to align with the real chat flow. If this component is meant for offline demo, clearly segregate it. Otherwise, consider removing the fake “demo mode” logic (lines 216-220) and have it call the actual useAdvancedChat just like the normal interface. At minimum, ensure it handles failures: e.g., wrap the sendMessage() call (line 208) in try/catch or check the returned promise to handle rejections. This will prevent the UI from doing nothing on errors in demo mode.
Remove stale references: Search the codebase for any references to now-nonexistent components (like ChatPane). For example, if some module tries to import ChatPane which was removed or renamed, update those imports to the current component (possibly DemoReadyChatInterface or a new ChatPanel component). The analysis noted that ChatPane.tsx is missing, which implies there might be outdated code expecting it. Clean these up to avoid runtime crashes. This might involve adjusting the layout to use the new chat interface component.
Final integration testing: After implementing the above, run the application end-to-end. Verify that:
Sending a message triggers the backend, returns a conversation ID, and the SSE connection opens with that ID.
The multi-agent steps (planning, research, writing) produce streaming events that the frontend receives and displays (check that events like planning_started or content tokens appear in the UI in real time).
File uploads now affect the response (e.g., the response references the file content if appropriate).
No errors appear in the console or logs related to missing components or unhandled promise rejections.
The demo interface (if used) does not break the app – ideally it should now just be another way to send messages through the same fixed pipeline.
By passing these tests, we ensure the agentic backend and chat UI frontend are fully re-integrated, providing a working real-time chat experience with streaming responses.
Additional Recommendations
Unified SSE Channel Strategy: Adopt a single SSE dispatch mechanism for all agent events. We introduced an SSEService singleton to manage publishing, which uses one Redis channel per conversation. This strategy allows all agents in the pipeline to publish to the same channel, and the frontend to listen to that channel via one SSE connection. It’s scalable to multi-agent scenarios because each event is tagged (e.g. type: 'research_progress' vs 'content') rather than using separate channels. This unified channel approach avoids race conditions and ensures ordering of events as the user sees them. It also simplifies adding new agent types: they just emit an SSE event via the service with a new type, and the frontend can handle it if needed. Trace ID vs Conversation ID: Standardize on one identifier across the system. We recommend using conversation_id everywhere for clarity (since it represents the conversation/session). The backend should always return conversation_id in API responses (deprecating trace_id if it was an older naming). The frontend code should treat this as the key for both retrieving past messages and opening SSE streams. By propagating the same conversation_id through request/response and events, we eliminate confusion and bugs due to mismatched IDs. In practice, after these fixes, a new chat will receive a conversation ID from the backend, store it in state, and use it for all subsequent streaming and follow-up queries. Next.js Proxy vs Direct SSE: In a production environment with a separate backend server, it’s usually safer to use the Next.js API routes as a proxy for SSE to avoid CORS and leverage the same domain. Our plan ensures the relative /api/stream/[id] route on Next.js will pipe events from the backend. This is suitable for deployment (just make sure to set BACKEND_URL in Next.js to the internal or public address of the FastAPI service). In local development, you could bypass the proxy and connect directly to http://localhost:8000/api/stream/[id] (setting NEXT_PUBLIC_API_BASE_URL accordingly), but it’s critical that the environment variables are in sync to prevent mixed strategies. Whichever approach is chosen, be consistent: if using the proxy, always use the proxy (remove direct client URL constructions); if using direct, ensure CORS is enabled on the backend for the frontend origin. Given the complexity observed, our recommendation is to use the Next.js SSR proxy for streaming in production, and configure it properly, since it centralizes the configuration and avoids client-side cross-origin issues. The direct SSE can be reserved for debugging or trusted same-origin setups. This clarity in deployment strategy will prevent the kind of issues where one environment works and another silently fails due to connection differences. By following this comprehensive plan – grouping the fixes into API routing, SSE streaming, and orchestration layers – the HandyWriterzAI application will be restored to full functionality. The end result will be an end-to-end working system where a user can upload files, ask a question, and observe the multi-agent AI brainstorm and answer in real-time via streaming updates, all backed by a robust integration between the Next.js frontend and FastAPI backend.